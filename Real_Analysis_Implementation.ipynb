{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a64a9089-6b57-4842-b0ee-30c89801970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from evaluator_Linear import evaluator\n",
    "from probLearner import PMLearner, RewardLearner, PALearner\n",
    "from ratioLearner import  RatioLinearLearner as RatioLearner\n",
    "from qLearner_Linear import Qlearner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5990b442-7e34-4c1b-a7e8-b7f43bc52e66",
   "metadata": {},
   "source": [
    "## STEP0ï¼š Define the Target Policy and the Control Policy\n",
    "\n",
    "Modify the Policy.py file accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3fdbca5-1084-4af4-a243-aaa949f86b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Policy import target_policy, control_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57003822-e3e7-4a51-8a9b-654f7b315693",
   "metadata": {},
   "source": [
    "## STEP1: prepare the dataset\n",
    "\n",
    "The following is an example of a proper input dataset with 2 trajectories and 3 observations for each trajectory, which is a dictionary with keys:\n",
    "- s0: stacked initial states of all the trajectories, initial state, 2d-array\n",
    "- state: stacked states of all the trajectories at all time points, 2d-array\n",
    "- action: stacked sequence of actions for all trajectories at all time points, 1d-array\n",
    "- mediator: stacked mediators of all the trajectories at all time points, 2d-array\n",
    "- reward: stacked sequence of rewards for all trajectories at all time points, 1d-array\n",
    "- next_state: stacked next_states of all the trajectories at all time points, 2d-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc7b285f-7464-49b3-a921-35a119505d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Simulator import Simulator\n",
    "#dim_state=3; dim_mediator = 2\n",
    "#simulator = Simulator(model_type='Gaussian_semi', dim_state=dim_state, dim_mediator = dim_mediator)\n",
    "#simulator.sample_trajectory(num_trajectory=30, num_time=30, seed=0)\n",
    "#simulator.trajectory2iid()\n",
    "#sim_iid_dataset = simulator.iid_dataset\n",
    "#dataset = sim_iid_dataset\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95895c5c-bdbb-474d-8c57-720b7b7de20c",
   "metadata": {},
   "source": [
    "## STEP2: Modify the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f814d1ed-fe4b-4c3c-9569-c03b431b290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed hyper-parameter--no need to modify\n",
    "expectation_MCMC_iter = 100\n",
    "expectation_MCMC_iter_Q3 = expectation_MCMC_iter_Q_diff = 100\n",
    "truncate = 50\n",
    "problearner_parameters = {\"splitter\":[\"best\",\"random\"], \"max_depth\" : range(1,50)},\n",
    "\n",
    "#hyperparameters that need modification\n",
    "#dim_state = the dimension of the state variable\n",
    "#dim_meditor = the dimension of the mediator variable\n",
    "#ratio_ndim = number of features used to learn the ratio model # can be modified accordingly \n",
    "                #to learn how the ratio_ndim affect the estimation performance\n",
    "dim_state=3; dim_mediator = 2\n",
    "\n",
    "ratio_ndim = 15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d8e5e1-b6cd-4503-a032-b0fee8833112",
   "metadata": {},
   "source": [
    "## STEP3: Causal Effect Estimation (target policy = behavior policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6adad2f5-1d07-4724-9c54-cb6b4c3a43aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward {'max_depth': 3, 'splitter': 'best'}\n",
      "2.064634207711125\n"
     ]
    }
   ],
   "source": [
    "def target_policy_2(state, dim_state = 1, action=None, matrix_based=False):\n",
    "    if not matrix_based:\n",
    "        pa = .5\n",
    "\n",
    "        prob_arr = np.array([1-pa, pa])\n",
    "\n",
    "        if action is None:\n",
    "            action_value = np.random.choice([0, 1], 1, p=prob_arr)\n",
    "        else:\n",
    "            action_value = np.array([prob_arr[int(action)]])\n",
    "    else:\n",
    "        state1 = state.reshape((-1, dim_state))\n",
    "        action1 = np.copy(action).flatten()\n",
    "        pa = .5 * np.ones((state1.shape[0])).flatten()\n",
    "        action_value = pa * action1 + (1-pa) * (1-action1)\n",
    "    return action_value\n",
    "\n",
    "ratio_ndim = 15\n",
    "d = 3\n",
    "L = 7\n",
    "est_obj1 = evaluator(dataset,\n",
    "                     Qlearner, PMLearner, \n",
    "                     RewardLearner, PALearner, RatioLearner,\n",
    "                     problearner_parameters = problearner_parameters,\n",
    "                     ratio_ndim = ratio_ndim, truncate = truncate, l2penalty = 1.0,\n",
    "                     target_policy=target_policy_2, control_policy = control_policy, \n",
    "                     dim_state = dim_state, dim_mediator = dim_mediator, \n",
    "                     Q_settings = {'scaler': 'Identity','product_tensor': True, 'beta': 3/7, \n",
    "                                   'include_intercept': False, 'expectation_MCMC_iter_Q3': expectation_MCMC_iter_Q3, \n",
    "                                   'expectation_MCMC_iter_Q_diff':expectation_MCMC_iter_Q_diff, \n",
    "                                   'penalty': 10**(-9),'d': d, 'min_L': L},\n",
    "                     expectation_MCMC_iter = expectation_MCMC_iter,\n",
    "                     seed = 10)\n",
    "\n",
    "est_obj1.estimate_DE_ME_SE()\n",
    "est_value1 = est_obj1.est_DEMESE\n",
    "var_value1 = est_obj1.var_DEMESE\n",
    "\n",
    "\n",
    "#The following are the estimations of our interest\n",
    "\n",
    "#1. estimation used the proposed triply robust estimator\n",
    "DE_TR, ME_TR, SE_TR = est_value1[:3]\n",
    "\n",
    "#2. estimation used the direct estimator of etas\n",
    "DE_Direct, ME_Direct, SE_Direct = est_value1[3:6]\n",
    "\n",
    "#3. estimation used the baseline method\n",
    "DE_base, ME_base = est_value1[6:8]\n",
    "SE_base= np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9ab8b4-f8e1-4ab4-b852-c62c7d222ed0",
   "metadata": {},
   "source": [
    "# Try different dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cedfc1bd-076b-4d8f-8cfa-547aec76c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "NT = len(dataset['state'])\n",
    "max_L = int(np.sqrt((NT)**(3/7)))\n",
    "for ration_ndim in np.arange(5,25,2):\n",
    "    for d in [1,2,3]:\n",
    "        for L in np.arange(d+2, d+7):\n",
    "            est_obj1 = evaluator(dataset,\n",
    "                                 Qlearner, PMLearner, \n",
    "                                 RewardLearner, PALearner, RatioLearner,\n",
    "                                 problearner_parameters = problearner_parameters,\n",
    "                                 ratio_ndim = ratio_ndim, truncate = truncate, l2penalty = 1.0,\n",
    "                                 target_policy=target_policy_2, control_policy = control_policy, \n",
    "                                 dim_state = dim_state, dim_mediator = dim_mediator, \n",
    "                                 Q_settings = {'scaler': 'Identity','product_tensor': True, 'beta': 3/7, \n",
    "                                               'include_intercept': False, 'expectation_MCMC_iter_Q3': expectation_MCMC_iter_Q3, \n",
    "                                               'expectation_MCMC_iter_Q_diff':expectation_MCMC_iter_Q_diff, \n",
    "                                               'penalty': 10**(-9),'d': d, 'min_L': L},\n",
    "                                 expectation_MCMC_iter = expectation_MCMC_iter,\n",
    "                                 seed = 10)\n",
    "\n",
    "            est_obj1.estimate_DE_ME_SE()\n",
    "            est_value1 = est_obj1.est_DEMESE\n",
    "            var_value1 = est_obj1.var_DEMESE\n",
    "\n",
    "\n",
    "            #The following are the estimations of our interest\n",
    "\n",
    "            #1. estimation used the proposed triply robust estimator\n",
    "            DE_TR, ME_TR, SE_TR = est_value1[:3]\n",
    "\n",
    "            #2. estimation used the direct estimator of etas\n",
    "            DE_Direct, ME_Direct, SE_Direct = est_value1[3:6]\n",
    "\n",
    "            #3. estimation used the baseline method\n",
    "            DE_base, ME_base = est_value1[6:8]\n",
    "            SE_base= np.nan\n",
    "            print(ration_ndim, d, L)\n",
    "            real_L = max(L, max_L+d)\n",
    "            out.append([ration_ndim, d, real_L, DE_TR, ME_TR, SE_TR, DE_Direct, ME_Direct, SE_Direct, DE_base, ME_base,SE_base])\n",
    "\n",
    "            \n",
    "            \n",
    "import pandas as pd\n",
    "out = pd.DataFrame(out, columns = ['ration_ndim', 'd', 'real_L', 'DE_TR', 'ME_TR', 'SE_TR', \n",
    "                                   'DE_Direct', 'ME_Direct', 'SE_Direct', 'DE_base', 'ME_base','SE_base'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
