{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "282ddc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from evaluator_Linear import evaluator\n",
    "from probLearner import PMLearner, RewardLearner, PALearner\n",
    "from ratioLearner import  RatioLinearLearner as RatioLearner\n",
    "from qLearner_Linear import Qlearner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f4c22",
   "metadata": {},
   "source": [
    "1. Data structure update: add a new key \"time_idx\"; need to specify the num_trajectory and num_time (make sure that the data is structured as $[Obs_{00},Obs_{01},\\cdots,Obs_{NT}]$)\n",
    "2. three target policies are of interest\n",
    "    - (**Important!**) optimal policy (need to learn the behavior policy first, and save as a pickle file) (t_depend_target = False)\n",
    "    - (**Important!**) behavior policy (t_depend_target = False)\n",
    "    - contrast policies at different time t (t_depend_target = True)\n",
    "3. two versions of state are of interest\n",
    "    - $S_t = [mood_{t-1}]$\n",
    "    - (not consider for now) $S_t = [mood_{t-1}, step_{t-1}, sleep_{t-1}]$\n",
    "4. two versions of Q models are of interest\n",
    "    - Q = f([1,s]) (t_dependent_Q = False)\n",
    "    - Q = f([1,s,t] (t_dependent_Q = True)\n",
    "5. other parameters can be modified to improve the performance\n",
    "    - ratio_ndim = 15 (for S=mood, lower the ratio_ndim)\n",
    "    - d = 3\n",
    "    - L = 7\n",
    "    - scaler: 'Identity',\"NormCdf\",\"Standardize\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae037d7",
   "metadata": {},
   "source": [
    "## STEP0ï¼š Define the Target Policy and the Control Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d780dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control Policy\n",
    "def control_policy(state = None, dim_state=None, action=None, get_a = False):\n",
    "    # fixed policy with fixed action 0\n",
    "    if get_a:\n",
    "        action_value = np.array([0])\n",
    "    else:\n",
    "        state = np.copy(state).reshape(-1,dim_state)\n",
    "        NT = state.shape[0]\n",
    "        if action is None:\n",
    "            action_value = np.array([0]*NT)\n",
    "        else:\n",
    "            action = np.copy(action).flatten()\n",
    "            if len(action) == 1 and NT>1:\n",
    "                action = action * np.ones(NT)\n",
    "            action_value = 1-action\n",
    "    return action_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567abe1-138e-4694-a9b8-7cd04813c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_policy 1: Optimal Policy Learned from Observational Data\n",
    "from qbehavior import Learn_Behavior_Q\n",
    "from _util import *\n",
    "import pickle\n",
    "#\"Q_behavior.pickle\" is the file name we saved the behavior Q estimations\n",
    "Q_behavior = pickle.load(open(\"Q_behavior.pickle\", \"rb\", -1))\n",
    "def optimal_policy(state = None, dim_state = 1, action=None): \n",
    "    opt_A = Q_behavior.opt_A(state) \n",
    "    if action is None:\n",
    "        action_value = opt_A\n",
    "    else:\n",
    "        action = np.copy(action).flatten()\n",
    "        action_value = 1-abs(opt_A-action) \n",
    "    return action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e048b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_policy 2: Series of t-dependent policies, i.e., policy_0 = [1,0,0,0,.....]\n",
    "# set trt_step as 0/1/2/3/..../24\n",
    "trt_step = 0\n",
    "def contrast_policy_1(state = None, dim_state = 1, action=None, time_idx = None):\n",
    "    time_idx = np.copy(time_idx).reshape((-1,1))\n",
    "    NT = time_idx.shape[0]\n",
    "    A = 0+(time_idx == trt_step)\n",
    "    A = A.flatten()\n",
    "    if action is None:\n",
    "        action_value = A\n",
    "    else:\n",
    "        action = np.copy(action).flatten()\n",
    "        action_value = 1-abs(A-action) \n",
    "    return action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b37403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_policy 3: behavior policy\n",
    "def behavior_policy(state, dim_state = 1, action=None):\n",
    "    state = np.copy(state).reshape((-1, dim_state))\n",
    "    NT = state.shape[0]\n",
    "    pa = .75 * np.ones(NT)\n",
    "    if action is None:\n",
    "        if NT == 1:\n",
    "            pa = pa[0]\n",
    "            prob_arr = np.array([1-pa, pa])\n",
    "            action_value = np.random.choice([0, 1], 1, p=prob_arr)\n",
    "        else:\n",
    "            raise ValueError('No random for matrix input')\n",
    "    else:\n",
    "        action = np.copy(action).flatten()\n",
    "        action_value = pa * action + (1-pa) * (1-action)\n",
    "    return action_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ab462d",
   "metadata": {},
   "source": [
    "## STEP1: prepare the dataset\n",
    "\n",
    "The following is an example of a proper input dataset with 2 trajectories and 3 observations for each trajectory, which is a dictionary with keys:\n",
    "- s0: stacked initial states of all the trajectories, initial state, 2d-array\n",
    "- state: stacked states of all the trajectories at all time points, 2d-array\n",
    "- action: stacked sequence of actions for all trajectories at all time points, 1d-array\n",
    "- mediator: stacked mediators of all the trajectories at all time points, 2d-array\n",
    "- reward: stacked sequence of rewards for all trajectories at all time points, 1d-array\n",
    "- next_state: stacked next_states of all the trajectories at all time points, 2d-array\n",
    "- **time_idx: stacked time step index, 1d-array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f8085ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Simulator import Simulator\n",
    "#dim_state=1; dim_mediator = 2\n",
    "#std_M = 2; std_S = 2\n",
    "#simulator = Simulator(model_type='Gaussian_semi', dim_state=dim_state, \n",
    "#                      dim_mediator = dim_mediator, std_M = std_M, std_S = std_S)\n",
    "#simulator.sample_trajectory(num_trajectory=30, num_time=30, seed=0)\n",
    "#simulator.trajectory2iid()\n",
    "#sim_iid_dataset = simulator.iid_dataset\n",
    "#dataset = sim_iid_dataset\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7e6224",
   "metadata": {},
   "source": [
    "## STEP2: Modify the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cd64ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed hyper-parameter--no need to modify\n",
    "expectation_MCMC_iter = 50\n",
    "expectation_MCMC_iter_Q3 = expectation_MCMC_iter_Q_diff = 50\n",
    "truncate = 50\n",
    "problearner_parameters = {\"splitter\":[\"best\",\"random\"], \"max_depth\" : range(1,50)},\n",
    "\n",
    "#hyperparameters that need modification\n",
    "#dim_state = the dimension of the state variable\n",
    "#dim_meditor = the dimension of the mediator variable\n",
    "#ratio_ndim = number of features used to learn the ratio model # can be modified accordingly \n",
    "                #to learn how the ratio_ndim affect the estimation performance\n",
    "dim_state=1; dim_mediator = 2\n",
    "\n",
    "ratio_ndim = 15\n",
    "d = 3\n",
    "L = 7\n",
    "\n",
    "t_depend_target = False\n",
    "target_policy= behavior_policy\n",
    "control_policy = control_policy\n",
    "t_dependent_Q = False\n",
    "scaler = 'Identity'\n",
    "num_trajectory = 30\n",
    "num_time = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7039d8",
   "metadata": {},
   "source": [
    "## STEP3: Causal Effect Estimation (target policy = behavior policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86274a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 0-th basis spline (total 3 state-mediator dimemsion) which has 3 basis, in total 3 features \n",
      "Building 1-th basis spline (total 3 state-mediator dimemsion) which has 3 basis, in total 6 features \n",
      "Building 2-th basis spline (total 3 state-mediator dimemsion) which has 3 basis, in total 9 features \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.50883687e-01, 1.81103125e-01, 1.30194736e-01, 1.38430969e-01,\n",
       "       2.43237678e-16, 1.62158452e-16, 4.05396130e-17, 0.00000000e+00,\n",
       "       4.31876076e-01, 2.75846953e-01, 6.92016723e-02, 1.01059660e-01,\n",
       "       1.62981134e-01, 2.81397155e-01, 4.44346548e-02, 2.78994450e-02])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_obj1 = evaluator(dataset, num_trajectory, num_time,\n",
    "                     Qlearner, PMLearner, RewardLearner, PALearner, RatioLearner,\n",
    "                     problearner_parameters = problearner_parameters,\n",
    "                     ratio_ndim = ratio_ndim, truncate = truncate, l2penalty = 10**(-4),\n",
    "                     t_depend_target = t_depend_target,\n",
    "                     target_policy=target_policy, control_policy = control_policy, \n",
    "                     dim_state = dim_state, dim_mediator = dim_mediator, \n",
    "                     Q_settings = {'scaler': scaler,'product_tensor': False, 'beta': 3/7, \n",
    "                                   'include_intercept': False, 'expectation_MCMC_iter_Q3': expectation_MCMC_iter_Q3, \n",
    "                                   'expectation_MCMC_iter_Q_diff':expectation_MCMC_iter_Q_diff, \n",
    "                                   'penalty': 10**(-4),'d': d, 'min_L': L, \"t_dependent_Q\": t_dependent_Q},\n",
    "                     expectation_MCMC_iter = expectation_MCMC_iter,\n",
    "                     seed = 10)\n",
    "\n",
    "est_obj1.estimate_DE_ME_SE()\n",
    "est_value1 = est_obj1.est_DEMESE\n",
    "se_value1 = est_obj1.se_DEMESE\n",
    "\n",
    "\n",
    "#The following are the estimations of our interest\n",
    "\n",
    "#1. estimation used the proposed triply robust estimator\n",
    "IDE_MR, IME_MR, DDE_MR, DME_MR = est_value1[:4]\n",
    "\n",
    "#2. estimation used the direct estimator of etas\n",
    "IDE_Direct, IME_Direct, DDE_Direct, DME_Direct = est_value1[4:8]\n",
    "\n",
    "#3. estimation used the WIS1 estimator of etas\n",
    "IDE_WIS1, IME_WIS1, DDE_WIS1, DME_WIS1 = est_value1[8:12]\n",
    "\n",
    "#4. estimation used the WIS2 estimator of etas\n",
    "IDE_WIS2, IME_WIS2 = est_value1[12:14]\n",
    "\n",
    "#5. estimation used the baseline method\n",
    "IDE_baseline, IME_baseline = est_value1[14:16]\n",
    "\n",
    "#6. SE of each estimator\n",
    "se_value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e33676-dfbb-4f66-aac2-3ed421a829e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
