{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "474b27cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from evaluator_Linear import evaluator\n",
    "from probLearner import PMLearner, RewardLearner, PALearner\n",
    "from ratioLearner import  RatioLinearLearner as RatioLearner\n",
    "from qLearner_Linear import Qlearner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e27fe97",
   "metadata": {},
   "source": [
    "1. Data structure update: add a new key \"time_idx\"\n",
    "2. three target policies are of interest\n",
    "    - optimal policy (need to learn the behavior policy first, and save as a pickle file) (t_depend_target = False)\n",
    "    - contrast policies at different time t (t_depend_target = True)\n",
    "    - behavior policy (t_depend_target = False)\n",
    "3. two versions of state are of interest\n",
    "    - $S_t = [mood_t]$\n",
    "    - $S_t = [mood_t, step_{t-1}, sleep_{t-1}]$\n",
    "4. two versions of Q models are of interest\n",
    "    - Q = f([1,s]) (t_dependent_Q = False)\n",
    "    - Q = f([1,s,t,st] (t_dependent_Q = True)\n",
    "5. other parameters can be modified to improve the performance\n",
    "    - ratio_ndim = 15\n",
    "    - d = 3\n",
    "    - L = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4db08f",
   "metadata": {},
   "source": [
    "## STEP0ï¼š Define the Target Policy and the Control Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cc26e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control Policy\n",
    "def control_policy(state = None, dim_state=None, action=None, get_a = False):\n",
    "    # fixed policy with fixed action 0\n",
    "    if get_a:\n",
    "        action_value = np.array([0])\n",
    "    else:\n",
    "        state = np.copy(state).reshape(-1,dim_state)\n",
    "        NT = state.shape[0]\n",
    "        if action is None:\n",
    "            action_value = np.array([0]*NT)\n",
    "        else:\n",
    "            action = np.copy(action).flatten()\n",
    "            if len(action) == 1 and NT>1:\n",
    "                action = action * np.ones(NT)\n",
    "            action_value = 1-action\n",
    "    return action_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef59d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_policy 1: Optimal Policy Learned from Observational Data\n",
    "from qbehavior import Learn_Behavior_Q\n",
    "from _util import *\n",
    "import pickle\n",
    "#\"Q_behavior.pickle\" is the file name we saved the behavior Q estimations\n",
    "Q_behavior = pickle.load(open(\"Q_behavior.pickle\", \"rb\", -1))\n",
    "def optimal_policy(state = None, dim_state = 1, action=None): \n",
    "    opt_A = Q_behavior.opt_A(state) \n",
    "    if action is None:\n",
    "        action_value = opt_A\n",
    "    else:\n",
    "        action = np.copy(action).flatten()\n",
    "        action_value = 1-abs(opt_A-action) \n",
    "    return action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_policy 2: Series of t-dependent policies, i.e., policy_0 = [1,0,0,0,.....]\n",
    "# set trt_step as 0/1/2/3/..../24\n",
    "trt_step = 0\n",
    "def contrast_policy_1(state = None, dim_state = 1, action=None, time_idx = None):\n",
    "    time_idx = np.copy(time_idx).reshape((-1,1))\n",
    "    NT = time_idx.shape[0]\n",
    "    A = 0+(time_idx == trt_step)\n",
    "    A = A.flatten()\n",
    "    if action is None:\n",
    "        action_value = A\n",
    "    else:\n",
    "        action = np.copy(action).flatten()\n",
    "        action_value = 1-abs(A-action) \n",
    "    return action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c27e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_policy 3: behavior policy\n",
    "def behavior_policy(state, dim_state = 1, action=None):\n",
    "    state = np.copy(state).reshape((-1, dim_state))\n",
    "    NT = state.shape[0]\n",
    "    pa = .75 * np.ones(NT)\n",
    "    if action is None:\n",
    "        if NT == 1:\n",
    "            pa = pa[0]\n",
    "            prob_arr = np.array([1-pa, pa])\n",
    "            action_value = np.random.choice([0, 1], 1, p=prob_arr)\n",
    "        else:\n",
    "            raise ValueError('No random for matrix input')\n",
    "    else:\n",
    "        action = np.copy(action).flatten()\n",
    "        action_value = pa * action + (1-pa) * (1-action)\n",
    "    return action_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ceebd",
   "metadata": {},
   "source": [
    "## STEP1: prepare the dataset\n",
    "\n",
    "The following is an example of a proper input dataset with 2 trajectories and 3 observations for each trajectory, which is a dictionary with keys:\n",
    "- s0: stacked initial states of all the trajectories, initial state, 2d-array\n",
    "- state: stacked states of all the trajectories at all time points, 2d-array\n",
    "- action: stacked sequence of actions for all trajectories at all time points, 1d-array\n",
    "- mediator: stacked mediators of all the trajectories at all time points, 2d-array\n",
    "- reward: stacked sequence of rewards for all trajectories at all time points, 1d-array\n",
    "- next_state: stacked next_states of all the trajectories at all time points, 2d-array\n",
    "- **time_idx: stacked time step index, 1d-array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec18a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Simulator import Simulator\n",
    "#dim_state=3; dim_mediator = 2\n",
    "#simulator = Simulator(model_type='Gaussian_semi', dim_state=dim_state, dim_mediator = dim_mediator)\n",
    "#simulator.sample_trajectory(num_trajectory=30, num_time=30, seed=0)\n",
    "#simulator.trajectory2iid()\n",
    "#sim_iid_dataset = simulator.iid_dataset\n",
    "#dataset = sim_iid_dataset\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269c4ad",
   "metadata": {},
   "source": [
    "## STEP2: Modify the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08827c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed hyper-parameter--no need to modify\n",
    "expectation_MCMC_iter = 100\n",
    "expectation_MCMC_iter_Q3 = expectation_MCMC_iter_Q_diff = 100\n",
    "truncate = 50\n",
    "problearner_parameters = {\"splitter\":[\"best\",\"random\"], \"max_depth\" : range(1,50)},\n",
    "\n",
    "#hyperparameters that need modification\n",
    "#dim_state = the dimension of the state variable\n",
    "#dim_meditor = the dimension of the mediator variable\n",
    "#ratio_ndim = number of features used to learn the ratio model # can be modified accordingly \n",
    "                #to learn how the ratio_ndim affect the estimation performance\n",
    "dim_state=3; dim_mediator = 2\n",
    "\n",
    "ratio_ndim = 15\n",
    "d = 3\n",
    "L = 7\n",
    "\n",
    "t_depend_target = False\n",
    "target_policy= behavior_policy\n",
    "control_policy = control_policy\n",
    "t_dependent_Q = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807fb4f3",
   "metadata": {},
   "source": [
    "## STEP3: Causal Effect Estimation (target policy = behavior policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a27b632d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward {'max_depth': 3, 'splitter': 'best'}\n",
      "2.064634207711125\n"
     ]
    }
   ],
   "source": [
    "est_obj1 = evaluator(dataset,\n",
    "                     Qlearner, PMLearner, \n",
    "                     RewardLearner, PALearner, RatioLearner,\n",
    "                     problearner_parameters = problearner_parameters,\n",
    "                     ratio_ndim = ratio_ndim, truncate = truncate, l2penalty = 10**(-4),\n",
    "                     t_depend_target = t_depend_target,\n",
    "                     target_policy=target_policy, control_policy = control_policy, \n",
    "                     dim_state = dim_state, dim_mediator = dim_mediator, \n",
    "                     Q_settings = {'scaler': 'Identity','product_tensor': False, 'beta': 3/7, \n",
    "                                   'include_intercept': False, 'expectation_MCMC_iter_Q3': expectation_MCMC_iter_Q3, \n",
    "                                   'expectation_MCMC_iter_Q_diff':expectation_MCMC_iter_Q_diff, \n",
    "                                   'penalty': 10**(-4),'d': d, 'min_L': L, \"t_dependent_Q\": t_dependent_Q},\n",
    "                     expectation_MCMC_iter = expectation_MCMC_iter,\n",
    "                     seed = 10)\n",
    "\n",
    "est_obj1.estimate_DE_ME_SE()\n",
    "est_value1 = est_obj1.est_DEMESE\n",
    "var_value1 = est_obj1.var_DEMESE\n",
    "\n",
    "\n",
    "#The following are the estimations of our interest\n",
    "\n",
    "#1. estimation used the proposed triply robust estimator\n",
    "DE_TR, ME_TR, SE_TR = est_value1[:3]\n",
    "\n",
    "#2. estimation used the direct estimator of etas\n",
    "DE_Direct, ME_Direct, SE_Direct = est_value1[3:6]\n",
    "\n",
    "#3. estimation used the baseline method\n",
    "DE_base, ME_base = est_value1[6:8]\n",
    "SE_base= np.nan\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
